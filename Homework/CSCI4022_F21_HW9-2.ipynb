{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI4022 Homework 9; Matrices\n",
    "\n",
    "\n",
    "## Due Wednesday, December 8 at 11:59 pm to Canvas and Gradescope\n",
    "\n",
    "#### Submit this file as a .ipynb with *all cells compiled and run* to the associated dropbox.\n",
    "\n",
    "***\n",
    "\n",
    "Your solutions to computational questions should include any specified Python code and results as well as written commentary on your conclusions.  Remember that you are encouraged to discuss the problems with your classmates, but **you must write all code and solutions on your own**.\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Any relevant data sets should be available on Canvas. To make life easier on the graders if they need to run your code, do not change the relative path names here. Instead, move the files around on your computer.\n",
    "- If you're not familiar with typesetting math directly into Markdown then by all means, do your work on paper first and then typeset it later.  Here is a [reference guide](https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference) linked on Canvas on writing math in Markdown. **All** of your written commentary, justifications and mathematical work should be in Markdown.  I also recommend the [wikibook](https://en.wikibooks.org/wiki/LaTeX) for LaTex.\n",
    "- Because you can technically evaluate notebook cells is a non-linear order, it's a good idea to do **Kernel $\\rightarrow$ Restart & Run All** as a check before submitting your solutions.  That way if we need to run your code you will know that it will work as expected. \n",
    "- It is **bad form** to make your reader interpret numerical output from your code.  If a question asks you to compute some value from the data you should show your code output **AND** write a summary of the results in Markdown directly below your code. \n",
    "- 45 points of this assignment are in problems.  The remaining 5 are for neatness, style, and overall exposition of both code and text.\n",
    "- This probably goes without saying, but... For any question that asks you to calculate something, you **must show all work and justify your answers to receive credit**. Sparse or nonexistent work will receive sparse or nonexistent credit. \n",
    "- There is *not a prescribed API* for these problems.  You may answer coding questions with whatever syntax or object typing you deem fit.  Your evaluation will primarily live in the clarity of how well you present your final results, so don't skip over any interpretations!  Your code should still be commented and readable to ensure you followed the given course algorithm.\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Problem 1](#p1) | [Problem 2](#p2) | \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p1'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 1 (20 pts; Practice: Outliers)\n",
    "\n",
    "The file `nba2021pg.txt` includes all of the major basketball statistics from the 2020-2021 NBA season.  We're going to use it to determine whether \"outlierness\" of players correlated with their value to the team.  Load in the file and inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rk</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>FG</th>\n",
       "      <th>FGA</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Precious Achiuwa\\achiupr01</td>\n",
       "      <td>PF</td>\n",
       "      <td>21</td>\n",
       "      <td>MIA</td>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>737</td>\n",
       "      <td>6.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509</td>\n",
       "      <td>3.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jaylen Adams\\adamsja01</td>\n",
       "      <td>PG</td>\n",
       "      <td>24</td>\n",
       "      <td>MIL</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Steven Adams\\adamsst01</td>\n",
       "      <td>C</td>\n",
       "      <td>27</td>\n",
       "      <td>NOP</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>1605</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444</td>\n",
       "      <td>4.8</td>\n",
       "      <td>6.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bam Adebayo\\adebaba01</td>\n",
       "      <td>C</td>\n",
       "      <td>23</td>\n",
       "      <td>MIA</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>2143</td>\n",
       "      <td>7.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799</td>\n",
       "      <td>2.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>20.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>LaMarcus Aldridge\\aldrila01</td>\n",
       "      <td>C</td>\n",
       "      <td>35</td>\n",
       "      <td>TOT</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>674</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>LaMarcus Aldridge\\aldrila01</td>\n",
       "      <td>C</td>\n",
       "      <td>35</td>\n",
       "      <td>SAS</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>544</td>\n",
       "      <td>7.6</td>\n",
       "      <td>16.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838</td>\n",
       "      <td>1.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>LaMarcus Aldridge\\aldrila01</td>\n",
       "      <td>C</td>\n",
       "      <td>35</td>\n",
       "      <td>BRK</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>130</td>\n",
       "      <td>6.9</td>\n",
       "      <td>13.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>Ty-Shon Alexander\\alexaty01</td>\n",
       "      <td>SG</td>\n",
       "      <td>22</td>\n",
       "      <td>PHO</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>2.3</td>\n",
       "      <td>9.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.1</td>\n",
       "      <td>7.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rk                       Player Pos  Age   Tm   G  GS    MP   FG   FGA  \\\n",
       "0   1   Precious Achiuwa\\achiupr01  PF   21  MIA  61   4   737  6.1  11.1   \n",
       "1   2       Jaylen Adams\\adamsja01  PG   24  MIL   7   0    18  2.0  16.0   \n",
       "2   3       Steven Adams\\adamsst01   C   27  NOP  58  58  1605  4.2   6.9   \n",
       "3   4        Bam Adebayo\\adebaba01   C   23  MIA  64  64  2143  7.7  13.4   \n",
       "4   5  LaMarcus Aldridge\\aldrila01   C   35  TOT  26  23   674  7.5  15.8   \n",
       "5   5  LaMarcus Aldridge\\aldrila01   C   35  SAS  21  18   544  7.6  16.4   \n",
       "6   5  LaMarcus Aldridge\\aldrila01   C   35  BRK   5   5   130  6.9  13.3   \n",
       "7   6  Ty-Shon Alexander\\alexaty01  SG   22  PHO  15   0    47  2.3   9.2   \n",
       "\n",
       "   ...     FT%  ORB  DRB   TRB  AST  STL  BLK  TOV   PF   PTS  \n",
       "0  ...   0.509  3.6  6.6  10.2  1.4  1.0  1.4  2.1  4.4  14.8  \n",
       "1  ...     NaN  0.0  6.0   6.0  4.0  0.0  0.0  0.0  2.0   4.0  \n",
       "2  ...   0.444  4.8  6.8  11.5  2.5  1.2  0.9  1.7  2.5   9.8  \n",
       "3  ...   0.799  2.4  7.2   9.6  5.8  1.3  1.1  2.8  2.4  20.1  \n",
       "4  ...   0.872  1.0  5.3   6.3  2.6  0.6  1.5  1.4  2.5  18.8  \n",
       "5  ...   0.838  1.1  5.1   6.2  2.4  0.5  1.2  1.3  2.4  19.1  \n",
       "6  ...   1.000  0.6  6.1   6.6  3.6  0.8  3.0  1.9  3.0  17.7  \n",
       "7  ...   0.500  1.5  6.1   7.7  4.6  0.0  0.8  2.3  1.5   6.9  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../data/nba2021pg.txt')\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rk', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%',\n",
       "       '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'FT', 'FTA', 'FT%', 'ORB',\n",
       "       'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Processing\n",
    "\n",
    "Note that the data set creates **3** rows if a player played for 2 teams during the season: one for each team and a \"Total.\"  For (only) those players, modify the data frame and delete all rows **except** when \"Tm\"=\"TOT\" so each row becomes a unique player.\n",
    "\n",
    "For all future plots, you may also only use NBA players that played at least 1000 minutes `MP` (around 1/4 of the seasons' total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Outliers: Classical\n",
    "\n",
    "Bi)  In the older days of NBA statistics, the most common measure of a players' value were in the triple (Points per game, Rebounds per game, Assists per game).  Create new columns in your data frame that house these 3 values.\n",
    "\n",
    "Bii) Use your new columns in an $n \\times 3$ object and calculate the **Mahalanobis' distance** for each player.  Who are the largest 10 outliers?\n",
    "\n",
    "Biii) Use the `decision_function` method from a fit of SKLearn's `IsolationForest` library in the import statement above.  Who are the largest 10 outliers under an Isolation Forest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Outliers: Rate\n",
    "\n",
    "Ci)  These days, we often get better date that measures how effective or ineffective a player is *when they're on the court*.  These are known as *rate* statistics, and require us create \"per minute\" statistics for each player.  Create new columns that are (Points per 36 minutes, Rebounds per 36 minutes, Assists per 36 minutes) for each player.\n",
    "\n",
    "Cii) Use your new columns in an $n \\times 3$ object and calculate the **Mahalanobis' distance** for each player.  Who are the largest 10 outliers?\n",
    "\n",
    "Ciii) Use the `decision_function` method from a fit of SKLearn's `IsolationForest` library in the import statement above.  Who are the largest 10 outliers under an Isolation Forest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Outliers: Which is better?\n",
    "For whether or not our methods are extracting players that are of value to the team, consider two measurements presented below.\n",
    "\n",
    "First, consider the (ordered) list of final standings in the league MVP voting, found [here](https://www.basketball-reference.com/awards/awards_2021.html#mvp).  \n",
    "Second, consider the top 20 by \"win shares\", listed [here](https://www.basketball-reference.com/leagues/NBA_2021_advanced.html#advanced_stats::ws).\n",
    "\n",
    "Modern NBA thinking suggests that rate statistics are more useful than simple aggregates.  Do we seem to get a better top-10 list from rate statistics?  Are your outliers coming from the most valuable players list or are they outliers in possibly **not** valuable ways?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Visualizing Skill\n",
    "\n",
    "We're often interested in how well a player shoots.  The issue is that the best players are often tasked with shooting more to support their team.  As a result, their shots tend to be focused on by the defenses more.  A measure of an excellent shooter isn't just their percentage of makes, but it also accounts for that the best shooters have to also make harder shots!\n",
    "\n",
    "Let's create a plot that helps identify this:\n",
    "\n",
    "- Create a variable \"eFG%\" (effective FG%), which attempts to estimate the value a player provides when they shoot by combining 2-point shots and 3-point shots.\n",
    "$$eFG\\%= \\frac{(2P)+1.5\\cdot(3P)}{(FGA)}$$\n",
    "\n",
    "- Create a variable \"SP36\"  which is the number of shots (FGA) a player takes per 36 minutes player.\n",
    "\n",
    "- Create a scatter plot of (SP36, eFG%).  Find the points for Nikola Jokić (MVP!) and for (two of) the best outside scorers in the NBA: Kevin Durant, and Stephen Curry and color them differently.  Do they appear to be outliers?\n",
    "\n",
    "- Create a scatter plot of (MP, PTS).  As before, color the points for Nikola Jokić, Kevin Durant, and Stephen Curry differently.  Do they appear to be more of outlier in these measures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: Single Stat-Outliers\n",
    "\n",
    "Take your choice of the statistics available that you think might best reflect \"player quality.\"  Given that statistics value for players with over 1000 minutes played:\n",
    "\n",
    "- Use a Box Cox transformation to convert it \"approximately Normal\" (or at least as close as you can get it)\n",
    "- Once transformed, get a Z-score for each player.  Who are the top 5 players and their Z-scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a/ id='p2'></a>\n",
    "[Back to top](#top)\n",
    "# Problem 2 (25 pts; Practice: SVD)\n",
    "\n",
    "Suppose our goal is to create a song classifier that takes lyrics and attempts to group similar songs.  Load in `tokenized_songs.csv` and inspect it.\n",
    "\n",
    "The data was taken by tokenizing song lyrics from a variety of songs by 3 artists: 3 Doors Down, Britney Spears, and Rick Astley.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  filtered_song_name  10  10x  110  150  17  17year  1958  1963  1966  \\\n",
      "0   here without you   0    0    0    0   0       0     0     0     0   \n",
      "1       when im gone   0    0    0    0   0       0     0     0     0   \n",
      "\n",
      "       ...       youth  youve  youâ  yup  zeh  zeus  zing  zipper  zone  \\\n",
      "0      ...           0      0     0    0    0     0     0       0     0   \n",
      "1      ...           0      0     0    0    0     0     0       0     0   \n",
      "\n",
      "         artist  \n",
      "0  3-doors-down  \n",
      "1  3-doors-down  \n",
      "\n",
      "[2 rows x 4660 columns]\n",
      "(548, 4660)\n",
      "['3-doors-down' 'britney-spears' 'rick-astley']\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('../data/tokenized_songs.csv')\n",
    "print(df.head(2))\n",
    "print(df.shape)\n",
    "print(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: All or some?: Forming SVDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.linalg.svd` to create the SVD of the data.  Make sure to remove the first and last columns, since they're string-based indicators and use only the tokenized lyrics.\n",
    "\n",
    "Then run your line of code to create the SVD in `timeit()` to estimate the amount of time it takes Python to create the full SVD of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: All or some?:  Generalized Power Iteration\n",
    "\n",
    "Use generalized power iteration to find only the first 2 eigenvalue, eigenvector pairs of the data set.  Write this as a function that inputs the original data frame (minus the first and last rows) and\n",
    "- creates $A= M^TM$\n",
    "- Uses generalized power iteration on A until $L_1$ convergence for the principal eigenvector\n",
    "- Uses generalized power iteration on A2 until $L_1$ convergence for the second principal eigenvector\n",
    " \n",
    "Time this function, as you did for `np.linalg.svd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Compare\n",
    "\n",
    "Print the top two singular values and the top two eigenvalues by the two methods and ensure their eqiuvalence.\n",
    "\n",
    "Which method was faster?  We should maybe expect the generalized power iteration to perform less operations, but what kind of efficiencies might exist in the np.linalg.svd function that don't exist in our generalized power iteration?\n",
    "\n",
    "(Hints: consider sparsity, and look up `np.linalg.eigh`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Visualize Results\n",
    "\n",
    "Take the original data and rotate it into 2 dimensions corresponding to the dominant two word-concepts.  As a sanity check, your final plot should have 548 rows (each of which are coordinate pairs), since we've collapsed the 4558 \"word\" columns into their principal directions.\n",
    "\n",
    "Create a scatter plot of the data, labeling your axes \"PC1\" and \"PC2\" and coloring the data by the artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Inspect Results\n",
    "\n",
    "You should notice that one artist seems to be responsible for most of the outlying songs in the principal component directions.\n",
    "\n",
    "Ei) Let's create a very lazy measure for outlierness and say that any song that has a **sum** of PC1 and PC2 values exceeding 100 is an \"outlier,\" of which there should be 4 songs.  Which songs are they?\n",
    "\n",
    "Eii) What lyrics seem to be most responsible for the spread of the data in the reduced dimensions?  In other words: in these 4 songs what words were most common, and how often did they occur? Print out the word frequencies in the original data frame for any words that appeared 25 or more times in the 4 \"outlier\" songs.\n",
    "\n",
    "Eiii) More importantly: is there anything else we could or should have done when processing the songs to possibly prevent this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution Markdown**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F: The last homework problem of the class\n",
    "\n",
    "Take the original data\n",
    "Consider the song in index 501 of the original data frame.  What song is it?  How strong was it in the concept space in the 2-dimensional representation of the data?  Does it appear exceptional in any way?  Repeat your plot from part C with song \\#501 in its own color, with a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
