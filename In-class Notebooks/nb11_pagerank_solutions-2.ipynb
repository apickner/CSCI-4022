{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 11: PageRank Computation\n",
    "***\n",
    "\n",
    "In this notebook we'll have a closer look at some of the issues that can confound a naive calculation of PageRank (dead-ends and spider traps). But, we'll also compute PageRank in a way to solve these issues!\n",
    "\n",
    "We'll need numpy for this notebook, so let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 0: Following along in class\n",
    "\n",
    "The transition matrix $M$ for the spider trap example from class is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1/2, 1/2, 0],\n",
    "              [1/2, 0  , 0],\n",
    "              [0  , 1/2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we saw that the modified transition matrix, to account for the possible teleports out of dead ends and spider traps, is:\n",
    "$$A = \\beta M + (1-\\beta) \\left[\\dfrac{1}{N}\\right]_{N\\times N}$$\n",
    "\n",
    "where $N$ is the number of pages and $\\beta$ is the probability of following an actual link. We can construct $A$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.  7.  1.]\n",
      " [ 7.  1.  1.]\n",
      " [ 1.  7. 13.]]\n"
     ]
    }
   ],
   "source": [
    "beta = 0.8\n",
    "A = beta*M + (1-beta)*np.ones((3,3))/3\n",
    "# Check after multiplying by 15 since that's the common denominator in \n",
    "# the slides (otherwise they'd be decimals and hard to compare directly)\n",
    "print(A*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.214 0.153 0.633]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "r_old = np.repeat(1/3, 3)\n",
    "\n",
    "# powerfully iterate\n",
    "for _ in range(10):\n",
    "    r_new = np.matmul(A, r_old)\n",
    "    r_old = r_new\n",
    "\n",
    "# see if we agree with what we see on the slides...\n",
    "print(np.round(r_new,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sick!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Dead-ends and spider traps!\n",
    "\n",
    "Seen here are two graphs: one has a spider trap, and one has a dead end. As we saw in the lecture, both present issues if one attempts to use the \"vanilla\" PageRank calculation from last time.\n",
    "\n",
    "<img width=600px src=\"http://www.cs.colorado.edu/~anwo7157/home/resources/pagerank2.png\">\n",
    "\n",
    "**[1]** Identify which of these graphs has the spider trap and which has the dead end. Which page(s) are associated with each of those problematic structures?\n",
    "\n",
    "**[2]** Then, form a hypothesis about how the computed PageRanks will reflect the dead end and spider trap problems in these graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transition matrices for each graph\n",
    "M1 = np.array([[0, 0, 1/2, 1/3, 1/2],\n",
    "               [1, 0, 0  , 0  , 1/2],\n",
    "               [0, 0, 0  , 1/3, 0  ],\n",
    "               [0, 0, 1/2, 0  , 0  ],\n",
    "               [0, 1, 0  , 1/3, 0  ]])\n",
    "\n",
    "M2 = np.array([[0, 0, 1/2, 0, 1/3],\n",
    "               [1, 0, 0  , 0, 1/3],\n",
    "               [0, 0, 0  , 0, 1/3],\n",
    "               [0, 0, 1/2, 0, 0  ],\n",
    "               [0, 1, 0  , 0, 0  ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "$G_1$ has a spider trap, consisting of Pages 1, 2 and 5. $G_2$ has Page 4 as a dead end.\n",
    "\n",
    "As a result of the spider trap in $G_1$, all of its PageRank will belong to Pages 1, 2 and 5, and Pages 3 and 4 will have 0 rank. As a result of the dead end in $G_2$, it will \"leak out\" all of its PageRank, so eventually all Pages have 0 rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 20 iterations of power iteration to obtain estimates for the PageRanks for each graph using the methods from last time. That is, use power iteration on the un-altered $M$ matrices. Were your hypotheses regarding the PageRanks correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial rank vector guess\n",
    "n = M1.shape[0]\n",
    "r_old = np.repeat(1/n, n)\n",
    "\n",
    "# power iteration!\n",
    "#               /\\\n",
    "#               |\n",
    "# <-- your code goes here! -->\n",
    "#               |\n",
    "#              \\/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  0.200  0.400  0.033  0.033  0.333\n",
      " 2  0.194  0.367  0.011  0.017  0.411\n",
      " 3  0.217  0.400  0.006  0.006  0.372\n",
      " 4  0.191  0.403  0.002  0.003  0.402\n",
      " 5  0.203  0.392  0.001  0.001  0.404\n",
      " 6  0.203  0.405  0.000  0.000  0.392\n",
      " 7  0.196  0.399  0.000  0.000  0.405\n",
      " 8  0.203  0.399  0.000  0.000  0.399\n",
      " 9  0.199  0.402  0.000  0.000  0.399\n",
      "10  0.199  0.399  0.000  0.000  0.402\n",
      "11  0.201  0.400  0.000  0.000  0.399\n",
      "12  0.199  0.400  0.000  0.000  0.400\n",
      "13  0.200  0.400  0.000  0.000  0.400\n",
      "14  0.200  0.400  0.000  0.000  0.400\n",
      "15  0.200  0.400  0.000  0.000  0.400\n",
      "16  0.200  0.400  0.000  0.000  0.400\n",
      "17  0.200  0.400  0.000  0.000  0.400\n",
      "18  0.200  0.400  0.000  0.000  0.400\n",
      "19  0.200  0.400  0.000  0.000  0.400\n",
      "20  0.200  0.400  0.000  0.000  0.400\n",
      "21  0.200  0.400  0.000  0.000  0.400\n",
      "22  0.200  0.400  0.000  0.000  0.400\n",
      "23  0.200  0.400  0.000  0.000  0.400\n",
      "24  0.200  0.400  0.000  0.000  0.400\n",
      "25  0.200  0.400  0.000  0.000  0.400\n",
      "26  0.200  0.400  0.000  0.000  0.400\n",
      "27  0.200  0.400  0.000  0.000  0.400\n",
      "28  0.200  0.400  0.000  0.000  0.400\n",
      "29  0.200  0.400  0.000  0.000  0.400\n",
      "30  0.200  0.400  0.000  0.000  0.400\n",
      "31  0.200  0.400  0.000  0.000  0.400\n",
      "32  0.200  0.400  0.000  0.000  0.400\n",
      "33  0.200  0.400  0.000  0.000  0.400\n",
      "34  0.200  0.400  0.000  0.000  0.400\n",
      "35  0.200  0.400  0.000  0.000  0.400\n",
      "36  0.200  0.400  0.000  0.000  0.400\n",
      "37  0.200  0.400  0.000  0.000  0.400\n",
      "38  0.200  0.400  0.000  0.000  0.400\n",
      "39  0.200  0.400  0.000  0.000  0.400\n",
      "40  0.200  0.400  0.000  0.000  0.400\n",
      "41  0.200  0.400  0.000  0.000  0.400\n",
      "42  0.200  0.400  0.000  0.000  0.400\n",
      "43  0.200  0.400  0.000  0.000  0.400\n",
      "44  0.200  0.400  0.000  0.000  0.400\n",
      "45  0.200  0.400  0.000  0.000  0.400\n",
      "46  0.200  0.400  0.000  0.000  0.400\n",
      "47  0.200  0.400  0.000  0.000  0.400\n",
      "48  0.200  0.400  0.000  0.000  0.400\n",
      "49  0.200  0.400  0.000  0.000  0.400\n",
      "50  0.200  0.400  0.000  0.000  0.400\n",
      "Rank vector for G1:  [0.2 0.4 0.  0.  0.4]\n",
      "Rank vector for G2:  [0.0402 0.0691 0.0262 0.014  0.0737]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "# initial rank vector guess\n",
    "n = M1.shape[0]\n",
    "\n",
    "r = np.repeat(1/n, n)\n",
    "r_old = r.copy()\n",
    "r_new = np.matmul(M1, r_old)\n",
    "\n",
    "# power iteration for M1\n",
    "for iters in range(50):\n",
    "    r_old = r_new.copy()\n",
    "    r_new = np.matmul(M1, r_old)\n",
    "    print(\"{:2.0f}  {:0.3f}  {:0.3f}  {:0.3f}  {:0.3f}  {:0.3f}\".format(iters+1, r_new[0],r_new[1],r_new[2],r_new[3],r_new[4]))\n",
    "    \n",
    "print(\"Rank vector for G1: \", np.round(r_new, 4))\n",
    "\n",
    "# initial rank vector guess\n",
    "r_old = np.repeat(1/n, n)\n",
    "\n",
    "# power iteration for M2\n",
    "for _ in range(20):\n",
    "    r_new = np.matmul(M2, r_old)\n",
    "    r_old = r_new\n",
    "    \n",
    "print(\"Rank vector for G2: \", np.round(r_new, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider:** What do you think would happen to the PageRanks in the spider trap graph if there was no connection from Page 5 to Page 2? What about if there were no connection from Page 2 to Page 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Fixing the problems\n",
    "\n",
    "Recall the random walker interpretation of PageRank from last time: A page's rank is equal to the long-run proportion of time that the walker spends on that Page, if she moves from page to page following any available out-link uniformly at random. Note that in the case of spider traps and dead ends, she gets stuck, which is of course and issue when we want the walker to roam free and explore the web graph.\n",
    "\n",
    "We fix the issues of dead ends and spider traps by providing some probability $\\beta < 1$ that the walker follows an actual link (chosen uniformly at random from those available), and with probability $1-\\beta$, she teleports to a page chosen uniformly at random from some *teleport set* of pages. In general, the teleport set is taken to be the set of all pages, so the walker could pop up anywhere.\n",
    "\n",
    "This leads to the updated transition matrix (from the slides):\n",
    "$$A = \\beta M + (1-\\beta) \\left[\\dfrac{1}{N}\\right]_{N\\times N}$$\n",
    "\n",
    "where $N$ is the number of pages, and the term in brackets is an $N \\times N$ matrix full of $1/N$s.\n",
    "\n",
    "For the case of the graph with the spider trap, construct the modified transition matrix $A$, using $\\beta=0.85$ (which is a typical choice in real-life applications). Then do 20 steps of power iteration to check on our new and improved PageRank estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank vector for G1:  [0.2089 0.354  0.0438 0.0486 0.3447]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "beta = 0.85\n",
    "Nmat = np.ones((n,n))/n\n",
    "A1 = beta*M1 + (1-beta)*Nmat\n",
    "\n",
    "# initial rank vector guess\n",
    "r_old = np.repeat(1/n, n)\n",
    "\n",
    "# power iteration for M1\n",
    "for _ in range(20):\n",
    "    r_new = np.matmul(A1, r_old)\n",
    "    r_old = r_new\n",
    "    \n",
    "print(\"Rank vector for G1: \", np.round(r_new, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the addition of teleports fix the issue of accumulation of rank by the pages in the spider trap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: Sparse matrix encoding\n",
    "\n",
    "Turns out, there are LOTS of web pages. Crazy, right? That means the matrix $M$ is huge, but sparse. So, representing that in memory can be challenging, but not impossible. The transition matrix updated to include the teleports, $A$, on the other hand, is fully dense because any page is reachable from any other page. Thus, $A$ may well be impossible to store in memory.\n",
    "\n",
    "We decomposed the update equation for power iteration so that it would sequentially read in a single page's degree and out-link information, and update each of the out-linked nodes' ranks:\n",
    "* First, we initialize all entries in $\\vec{r}^{new}$ to equal $(1-\\beta)/n$\n",
    "* Then we loop over each page $i$ with out-degree $d_i$:\n",
    "  * For each destination page that page links to, we distribute (add) $\\beta r^{old}_i / d_i$ of rank\n",
    "\n",
    "Store the degree and destination page information in a list of lists, where the primary index for the list corresponds to the source node, the first element of each constituent list is a single integer for that node's (out-)degree, and the second element of each constituent list is a list of that source node's destination nodes. \n",
    "\n",
    "For example, the row corresponding to Page 4 would be the fourth element of our list:\n",
    "$$[3, [1, 3, 5]]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-e4882791ce02>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-e4882791ce02>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    M_compact = [[degree of 1, [destinations from 1]],\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO -- replace the words below with the appropriate numbers\n",
    "M_compact = [[degree of 1, [destinations from 1]],\n",
    "             [degree of 2, [destinations from 2]],\n",
    "             [degree of 3, [destinations from 3]],\n",
    "             [degree of 4, [destinations from 4]],\n",
    "             [degree of 5, [destinations from 5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "M_compact = [[1, [2]],\n",
    "             [1, [5]],\n",
    "             [2, [1,4]],\n",
    "             [3, [1,3,5]],\n",
    "             [2, [1,2]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize our old rank vector as all $1/n$, and the new rank vector that we will be computing to all $(1-\\beta)/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.85\n",
    "r_old = np.repeat(1/n, n)         # initializing the entire power iteration\n",
    "r_new = np.repeat((1-beta)/n, n)  # initializing the output from a single step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop over the rows of `M_compact` and distribute the importance. The update from the first row is done for you below. Your job is to turn it into a `for` loop over all of the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dest in M_compact[0][1]:\n",
    "    idx = dest-1 # accounting for Python's 0-based indexing\n",
    "    r_new[idx] += beta*r_old[0]/M_compact[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17\n",
      "0.17\n",
      "0.012750000000000004\n",
      "0.012750000000000004\n",
      "0.012112500000000005\n",
      "0.012112500000000005\n",
      "0.012112499999999998\n",
      "0.09014781250000001\n",
      "0.09014781249999998\n",
      "[0.145  0.2901 0.0421 0.0428 0.2121]\n",
      "[[1, [2]], [1, [5]], [2, [1, 4]], [3, [1, 3, 5]], [2, [1, 2]]]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "def chebyshev_d(x,y):\n",
    "    return np.max(np.abs(x-y))\n",
    "\n",
    "beta = 0.85\n",
    "r_old = np.repeat(1/n, n)\n",
    "r_new = np.repeat((1-beta)/n, n)\n",
    "\n",
    "for page in range(n):\n",
    "    for dest in M_compact[page][1]:\n",
    "        idx = dest-1 # accounting for Python's 0-based indexing\n",
    "        r_new[idx] += beta*r_old[page]/M_compact[page][0]\n",
    "        print(chebyshev_d(r_old, r_new))\n",
    "        r_old = r_new.copy()\n",
    "print(np.round(r_new, 4))\n",
    "print(M_compact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to a single step of the regular power iteration (with the random teleporting). Do they agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11333333333333331\n",
      "Rank:  [0.2567 0.285  0.0867 0.115  0.2567]\n",
      "0.07225000000000004\n",
      "Rank:  [0.2085 0.3573 0.0626 0.0668 0.3048]\n",
      "0.04776527777777784\n",
      "Rank:  [0.2051 0.3368 0.0489 0.0566 0.3526]\n",
      "0.0203002430555555\n",
      "Rank:  [0.2167 0.3542 0.046  0.0508 0.3323]\n",
      "0.013146824074074026\n",
      "Rank:  [0.2052 0.3554 0.0444 0.0496 0.3454]\n",
      "0.00453976268807868\n",
      "Rank:  [0.2097 0.3512 0.044  0.0489 0.3461]\n",
      "0.004155628922164312\n",
      "Rank:  [0.2097 0.3554 0.0438 0.0487 0.3424]\n",
      "0.003490233576889157\n",
      "Rank:  [0.208  0.3537 0.0438 0.0486 0.3459]\n",
      "0.0014416486882853352\n",
      "Rank:  [0.2094 0.3538 0.0438 0.0486 0.3445]\n",
      "0.0006228279766951617\n",
      "Rank:  [0.2088 0.3544 0.0438 0.0486 0.3445]\n",
      "0.0005265343830083147\n",
      "Rank:  [0.2088 0.3539 0.0438 0.0486 0.345 ]\n",
      "0.00044328599724796636\n",
      "Rank:  [0.209  0.3541 0.0438 0.0486 0.3445]\n",
      "0.00018934673775172772\n",
      "Rank:  [0.2088 0.3541 0.0438 0.0486 0.3447]\n",
      "8.025209247639054e-05\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "6.827668874082038e-05\n",
      "Rank:  [0.2089 0.3541 0.0438 0.0486 0.3447]\n",
      "5.8026343993766716e-05\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "2.465242844007509e-05\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.0479411399533234e-05\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "8.906896384441865e-06\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "7.569964510234506e-06\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "3.2174346987767777e-06\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.367363433896207e-06\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.1622720409443232e-06\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "9.879293758618957e-07\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "4.1986814128258665e-07\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.784444077501668e-07\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.516776197973968e-07\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.2892578810097532e-07\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "5.47935019579171e-08\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "2.3287228617663303e-08\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.979414704367244e-08\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.682502470679026e-08\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "7.150635139563377e-09\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "3.0390200911334375e-09\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "2.583167069136749e-09\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "2.195691906070607e-09\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "9.33169097550035e-10\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "3.965968664587649e-10\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "3.371073420410653e-10\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "2.865412351837904e-10\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.2178003050422603e-10\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "5.175659900658047e-11\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "4.399308695113291e-11\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "3.739403231506344e-11\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "1.5892454019450497e-11\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "6.754374837214527e-12\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "5.741240816092841e-12\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "4.880040815891107e-12\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "2.0740353878778706e-12\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n",
      "8.815170815523743e-13\n",
      "Rank:  [0.2089 0.354  0.0438 0.0486 0.3447]\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "beta = 0.85\n",
    "Nmat = np.ones((n,n))/n\n",
    "A1 = beta*M1 + (1-beta)*Nmat\n",
    "\n",
    "# initial rank vector guess\n",
    "r_old = np.repeat(1/n, n)\n",
    "\n",
    "# power iteration for M1\n",
    "for _ in range(50):\n",
    "    r_new = np.matmul(A1, r_old)\n",
    "    print(chebyshev_d(r_old, r_new))\n",
    "    r_old = r_new\n",
    "    print(\"Rank: \", np.round(r_new, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** They totally do agree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
