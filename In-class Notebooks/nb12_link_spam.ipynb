{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 12: Link Spam and Topic-Specific PageRank\n",
    "***\n",
    "\n",
    "In this notebook we'll examine the how topic-specific PageRank can be leveraged to weed out spam pages and diagnose spam.\n",
    "\n",
    "We'll need numpy for this notebook, so let's load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 0: Following along with lecture is cool!\n",
    "\n",
    "With the random teleports (to solve the spider traps and dead ends problems), our transition matrix $A$ is constructed of:\n",
    "$$A_{ij} = \\beta M_{ij} + (1-\\beta)/N$$\n",
    "where $M$ is the original transition matrix, $\\beta$ is a probability (typically between 0.8 and 0.9) of taking an actual out-link (as opposed to a teleport), and $N$ is the number of pages. These elements $A_{ij}$ are the probability of transitioning from page $j$ to page $i$ (possibly a bit sideways from what you might intuitively think, but it makes the power iteration work out).\n",
    "\n",
    "With ***topic-specific*** PageRank, however, we restrict the teleport set $S$ to only a subset of the pages (on the topic of interest). Our transition matrix becomes:\n",
    "$$A_{ij} = \\begin{cases} \\beta M_{ij} + (1-\\beta)/|S| & \\text{if } i \\in S \\\\ \\beta M_{ij} & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example of slide 10, we have the following graph:\n",
    "\n",
    "<img width=200px src=\"http://www.cs.colorado.edu/~anwo7157/home/resources/topicspecificpagerank1.png\">\n",
    "\n",
    "We saw that the transition matrix $M$ for this graph is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1/2, 1/3, 0, 1/2],\n",
    "              [1/2, 1/3, 0, 0  ],\n",
    "              [0  , 1/3, 0, 1/2],\n",
    "              [0  , 0  , 1, 0  ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\beta = 0.8$ and the teleport set $S = \\{1,2\\}$, we can make a slight modification of our previous code to make it topic-specific. In particular, in the matrix that is associated with the teleports (the $1-\\beta$ term in $A$), we want 1s in the rows corresponding to the teleport set's pages, and 0s in the other rows. In our original code, we have 1s everywhere. We also need to account for division by the size of the teleport set, $|S|$, instead of the full set of pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD code:\n",
    "# TODO -- can you cleverly modify the line with `Nmat`\n",
    "#         to only have 1s for the in-topic nodes? You can\n",
    "#         use more than 1 line if that makes life easier.\n",
    "n = 4\n",
    "beta = 0.8\n",
    "Nmat = np.ones((n,n))/n # TODO -- modify this line for topic-specific PageRank\n",
    "A = beta*M + (1-beta)*Nmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "n = 4\n",
    "beta = 0.8\n",
    "Nmat = np.zeros((n,n))\n",
    "S = [1, 2]\n",
    "for i in S:\n",
    "    Nmat[i-1,:] = np.ones(n)/len(S)\n",
    "A = beta*M + (1-beta)*Nmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate, we need to initialize our PageRank vector, which we can do using an even distribution of rank:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_old = np.repeat(1/n, n)\n",
    "r_new = np.repeat(1/n, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can perform 10 iterations of power iteration, and make sure we agree with the lecture results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"    1      2      3      4\")\n",
    "print(\"{:2.0f}  {:0.3f}  {:0.3f}  {:0.3f}  {:0.3f}\".format(0, r_old[0],r_old[1],r_old[2],r_old[3]))\n",
    "for t in range(10):\n",
    "    r_new = 0 # TODO -- compute new rank vector from the old one\n",
    "    r_old = 0 # TODO -- step the old one forward for the next iteration\n",
    "    # Renormalize r_new to make sure it sums to 1:\n",
    "    #   1) obtain the sum of the elements in r_new\n",
    "    mag = 0 #   TODO\n",
    "    #   2) divide each element of r_new by the sum of all of them\n",
    "    r_new = 0 # TODO\n",
    "    print(\"{:2.0f}  {:0.3f}  {:0.3f}  {:0.3f}  {:0.3f}\".format(t+1, r_new[0],r_new[1],r_new[2],r_new[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Spam farming\n",
    "\n",
    "Recall the original graph we looked at when learning power iteration (notebook 10).\n",
    "\n",
    "<img width=250px src=\"http://www.cs.colorado.edu/~anwo7157/home/resources/pagerank1.png\">\n",
    "\n",
    "Let's obtain the PageRanks for each of the 5 pages using a teleport probability of 0.15 ($\\beta = 0.85$). We'll use the compact representation of the transition matrix $M$, since this will lend itself well to expanding our web graph as the devious owner of Page 4 starts up a **spam farm**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the compact representation for M\n",
    "M_compact = [[Page 1 out-degree, [Page 1 destinations]],\n",
    "             [Page 2 out-degree, [Page 2 destinations]],\n",
    "             [Page 3 out-degree, [Page 2 destinations]],\n",
    "             [Page 4 out-degree, [Page 3 destinations]],\n",
    "             [Page 5 out-degree, [Page 5 destinations]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "N = 5\n",
    "M = np.zeros((N,N))\n",
    "M[:,0] = [0, 1, 0, 0, 0]\n",
    "M[:,1] = [0, 0, 0, 0, 1]\n",
    "M[:,2] = [1/2, 0, 0, 1/2, 0]\n",
    "M[:,3] = [1/3, 0, 1/3, 0, 1/3]\n",
    "M[:,4] = [0, 1/2, 1/2, 0, 0]\n",
    "print(M)\n",
    "\n",
    "M_compact = {}\n",
    "for i in range(N):\n",
    "    M_compact[i+1] = [idx+1 for idx, x in enumerate(M[:,i]) if x > 0]\n",
    "print(M_compact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stealing liberally from our own previous code, we can initialize and run 50 iterations of power iteration as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1372 0.389  0.0466 0.0382 0.389 ]\n"
     ]
    }
   ],
   "source": [
    "# initialize for power iteration\n",
    "n = len(M_compact)\n",
    "beta = 0.85\n",
    "r_old = np.repeat(1/n, n)\n",
    "\n",
    "for t in range(50):\n",
    "    # initialize with the \"taxed\" importance\n",
    "    r_new = np.repeat((1-beta)/n, n)\n",
    "    # distribute importance payments between the nodes\n",
    "    for page in range(n):\n",
    "        for dest in M_compact[page][1]:\n",
    "            idx = dest-1 # accounting for Python's 0-based indexing\n",
    "            r_new[idx] += beta*r_old[page]/M_compact[page][0]\n",
    "    # normalize to ensure r_new sums to 1\n",
    "    r_new = r_new/np.sum(r_new)\n",
    "    r_old = r_new\n",
    "\n",
    "print(np.round(r_new, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider:** What line would we need to change if we wanted to use *topic-specific* PageRank? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The devious owner of Page 4, Franklin, is sad that his page has such low rank. In an effort to boost it, he decides to create a **spam farm**. He doesn't want to go all in at once, so first Franklin creates a single spam page (Page 6), with a single out-link to Page 4, and a single in-link from Page 4.\n",
    "\n",
    "Append the link information for Page 6 to `M_compact`. Don't forget to update the link information for Page 4, as well as Page 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And recompute the PageRank for all of the pages. Don't forget to update the number of pages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0637 0.1395 0.0306 0.6439 0.0997 0.0227]\n"
     ]
    }
   ],
   "source": [
    "# initialize for power iteration\n",
    "n = len(M_compact)\n",
    "beta = 0.85\n",
    "r_old = np.repeat(1/n, n)\n",
    "\n",
    "for t in range(50):\n",
    "    # initialize with the \"taxed\" importance\n",
    "    r_new = np.repeat((1-beta)/n, n)\n",
    "    # distribute importance payments between the nodes\n",
    "    for page in range(n):\n",
    "        for dest in M_compact[page][1]:\n",
    "            idx = dest-1 # accounting for Python's 0-based indexing\n",
    "            r_new[idx] += beta*r_old[page]/M_compact[page][0]\n",
    "    # normalize to ensure r_new sums to 1\n",
    "    r_new = r_new/np.sum(r_new)\n",
    "    r_old = r_new\n",
    "\n",
    "print(np.round(r_new, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need the full PageRank for all of these pages later, so let's hold onto it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = r_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By how much did the rank of Page 4 increase as a result of this spam farm page? The following equation was derived in class as the theoretical increase in PageRank of a target page as a result of $M$ spam pages feeding into it:\n",
    "\n",
    "$$y = x + \\beta M \\left[\\dfrac{\\beta y}{M} + \\dfrac{1-\\beta}{N}\\right] + \\dfrac{1-\\beta}{N}$$\n",
    "\n",
    "where $y$ is the PageRank of the target page and $x$ is the rank contributed by good, upstanding pages. Is there some part of this equation that needs to be modified to fit our graph with the added spam page? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have found that the increase in PageRank for Franklin was large. Why did adding a single farm page yield such a huge benefit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Franklin's Treachery, and TrustRank, the hero we need!\n",
    "\n",
    "<img width=280px src=\"http://www.cs.colorado.edu/~anwo7157/home/resources/linkspam1.png\">\n",
    "\n",
    "Now that our world is crashing down around us because of the treachery of Franklin, we need to figure out how to combat his admittedly sad little spam farm. Use the teleport set $S = \\{1,2,3,4,5\\}$ and $\\beta = 0.85$ to perform 50 iterations of power iteration and estimate *TrustRank* with respect to the trusted set $S$ (the original pages). Note that you will likely need to change the \"initialize with the taxed importance\" step in the code we used previously, to account for teleports only to the trusted set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of Page 4 hasn't really decreased by very much, which might be discouraging. After all, if we haven't knocked Franklin down a peg, what is all this really about?\n",
    "\n",
    "But wait! We can compute the **spam mass** of each page as:\n",
    "$$m = \\dfrac{r - t}{r}$$\n",
    "where $r$ is the PageRank (including the spam farm page) and $t$ is the TrustRank (with respect to only the trusted set $S$).\n",
    "\n",
    "So our spam masses for each page are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammass = (pagerank - r_new)/pagerank\n",
    "print(spammass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about Franklin and his farm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
