{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 16: Recommender Systems\n",
    "***\n",
    "\n",
    "In this notebook we'll construct some user and item profiles, so we can make recommendations for sugary cereals that a young user might enjoy. We will also explore using the Scikit-learn package to represent text data as vectors of word counts in a relatively simple example.\n",
    "\n",
    "We'll need numpy and Pandas for this notebook, so let's load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Constructing a user profile\n",
    "\n",
    "<img width=180px src=\"https://ih1.redbubble.net/image.452912965.4213/flat,550x550,075,f.jpg\">\n",
    "\n",
    "Suppose Parker is looking for cereal recommendations so he grows up big, strong and full of sugar. Being a clever child, Parker keeps a log of the cereals he has eaten, how much he enjoyed them, and various characteristics he suspects affect his enjoyment of a cereal. If Parker has had the cereal in a given row, there is a nonzero rating in each column corresponding to a feature characteristic of that cereal. 0s correspond to a lack of a particular feature for a particular cereal (e.g., Fruity Pebbles are do not contain marshmallows and do not contain chocolate, and Count Chocula has both marshmallows and chocolate, and Parker rated it as a 4).\n",
    "\n",
    "$$\\begin{array}{c|ccc}\n",
    " \\text{Cereal}             & \\text{Marshmallows} & \\text{Fruity} & \\text{Chocolate} \\\\\n",
    "   \\hline\n",
    " \\text{Fruity Pebbles}     & 0   & 1   & 0   \\\\\n",
    " \\text{Trix}               & 0   & 2   & 0   \\\\\n",
    " \\text{Lucky Charms}       & 5   & 0   & 0   \\\\\n",
    " \\text{Chocolate Cheerios} & 0   & 0   & 2   \\\\\n",
    " \\text{Cocoa Puffs}        & 0   & 0   & 4   \\\\\n",
    " \\text{Count Chocula}      & 4   & 0   & 4   \\\\\n",
    " \\end{array}$$\n",
    "\n",
    "Let's construct a user profile for Parker.\n",
    "\n",
    "**First**, *normalize* his ratings utility matrix by subtracting from each nonzero entry Parker's mean rating. Note that the problem of finding the mean of *only the nonzero entries* of the ratings matrix is a bit tricky! Do **not** immediately take to the internet to try and solve the problem! Give it some thought to see if you can conquer this challenge. It builds character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 2. 0.]\n",
      " [5. 0. 0.]\n",
      " [0. 0. 2.]\n",
      " [0. 0. 4.]\n",
      " [4. 0. 4.]]\n",
      "     0    1    2\n",
      "0  0.0  1.0  0.0\n",
      "1  0.0  2.0  0.0\n",
      "2  5.0  0.0  0.0\n",
      "3  0.0  0.0  2.0\n",
      "4  0.0  0.0  4.0\n",
      "5  4.0  0.0  4.0\n"
     ]
    }
   ],
   "source": [
    "# Here is the raw ratings matrix as both Numpy and Pandas:\n",
    "M = np.array([[0.,1,0],[0,2,0],[5,0,0],[0,0,2],[0,0,4],[4,0,4]])\n",
    "print(M)\n",
    "dfM = pd.DataFrame(M)\n",
    "print(dfM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1     3\n",
       "2    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION:\n",
    "np.sum(dfM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next**, we can compute Parker's user profile, which has an average for each feature in his cereal database. That is, we want to compute the components $\\text{Parker}_{\\text{[Marshmallow]}}$, $\\text{Parker}_{\\text{[Fruity]}}$ and $\\text{Parker}_{\\text{[Chocolate]}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you conclude about Parker's cereal preferences based on his normalized user profile? What type of cereal do you think he would be most likely to enjoy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Making recommendations\n",
    "\n",
    "Now suppose two new cereals become available at Wegman's, the regional grocery store that will ruin every other grocery store for you forever. These cereals have binary values in the ratings array since we do not yet know what Parker's ratings will be, but we do know whether they possess the three features in the database:\n",
    "\n",
    "$$\\begin{array}{c|ccc}\n",
    " \\text{Cereal}             & \\text{Marshmallows} & \\text{Fruity} & \\text{Chocolate} \\\\\n",
    "   \\hline\n",
    " \\text{Cereal A}           & 1   & 1   & 0   \\\\\n",
    " \\text{Cereal B}           & 1   & 0   & 1\n",
    " \\end{array}$$\n",
    "\n",
    "Compute the cosine similarity between Parker's user profile and each of these two cereals. Which cereal should Wegman's send Parker coupons for?\n",
    "\n",
    "**First**, make a guess based on your human intuition and Parker's ratings matrix from **Exercise 2**.\n",
    "\n",
    "**Then**, perform the actual computation to check your intuition. While this might be a \"duh!\" kind of result, the whole point is to produce an algorithmic approach to make recommendations when the numbers of ratings and products will be far too large to rely on intuition alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: \n",
    "\n",
    "Consider a corpus of three documents, each containing just a few sentences, shown below.\n",
    "\n",
    "1. Call me Ishmael. Some years ago - never mind how long precisely - having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\n",
    "1. There was no possibility of taking a walk that day. We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further out-door exercise was now out of the question.\n",
    "1. You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter. That book was made by Mr. Mark Twain, and he told the truth, mainly. \n",
    "\n",
    "Ultimately, we want to compute the term frequency-inverse document frequency (TF-IDF) scores for each of the term-document pairs, and build up an item profile for each of the documents consisting of the terms with the highest TF-IDF scores.\n",
    "\n",
    "To remind you of some definitions, the term frequency is: \n",
    "$TF_{ij} = \\dfrac{f_{ij}}{\\max_k{f_{k,j}}}$ = term frequency of term (feature) $i$ in document (item) $j$\n",
    "\n",
    "and the inverse-document frequency is: \n",
    "$IDF_i = \\log{\\dfrac{N}{n_i}}$ = inverse document frequency of term $i$, where $N$ is total number of documents and $n_i$ is the number of documents that mention term $i$\n",
    "\n",
    "and the TF-IDF score is $w_{ij} = TF_{ij} \\times IDF_i$.\n",
    "\n",
    "We can represent each of these documents as a string, and combine them into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Call me Ishmael. Some years ago - never mind how long precisely - having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\",        \n",
    "        \"There was no possibility of taking a walk that day. We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further out-door exercise was now out of the question.\",\n",
    "        \"You don't know about me without you have read a book by the name of The Adventures of Tom Sawyer; but that ain't no matter. That book was made by Mr. Mark Twain, and he told the truth, mainly.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in order to compute the TF-IDF scores, we need to separate into individual words, ignore capitalization and punctuation, and possibly also ignore *stop words* like \"the\" and \"of\". We *could* write our own code for processing a string into a set of words and tallying up their counts in each document... but that amounts to a straightforward but tedious CSCI 1300 exercise in ASCII manipulation. Instead, let's learn how to leverage Scikit-learn's `CountVectorizer` methods!\n",
    "\n",
    "There are lots of options for the `CountVectorizer` constructor, which you can read more about [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize and fit the CountVectorizer to the document data\n",
    "# Builds a vocabulary based on the words in the docs\n",
    "#vec = CountVectorizer(stop_words=None)\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "vec.fit(docs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the vocabulary that was built. See if you can spot any potential issues (there are some)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ishmael': 17, 'years': 56, 'ago': 1, 'mind': 25, 'long': 21, 'precisely': 33, 'having': 15, 'little': 20, 'money': 26, 'purse': 34, 'particular': 30, 'shore': 41, 'thought': 45, 'sail': 39, 'watery': 52, 'world': 55, 'possibility': 32, 'taking': 44, 'walk': 50, 'day': 8, 'wandering': 51, 'leafless': 19, 'shrubbery': 42, 'hour': 16, 'morning': 27, 'dinner': 10, 'mrs': 29, 'reed': 38, 'company': 7, 'dined': 9, 'early': 13, 'cold': 6, 'winter': 54, 'wind': 53, 'brought': 4, 'clouds': 5, 'sombre': 43, 'rain': 36, 'penetrating': 31, 'door': 12, 'exercise': 14, 'question': 35, 'don': 11, 'know': 18, 'read': 37, 'book': 3, 'adventures': 0, 'tom': 47, 'sawyer': 40, 'ain': 2, 'matter': 24, 'mr': 28, 'mark': 23, 'twain': 49, 'told': 46, 'truth': 48, 'mainly': 22}\n"
     ]
    }
   ],
   "source": [
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Note that the word \"don't\" (for example) had the 't stripped away. If someone were to don their cape, then the words would be mixing their counts.\n",
    "\n",
    "<img width=300px src=\"https://csingalls.files.wordpress.com/2014/09/be-a-hero.jpg\">\n",
    "\n",
    "To solve this, we *should* preprocess by stripping the punctuation away. But we will plug ahead anyhow..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode the document using the `transform` method, and feeding in the documents. Note that we could fit the bag-of-words model (i.e., that vocabulary we built up in the previous step) on one set of documents, and then encode a different set of documents into word count vectors for that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vecs = vec.transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we just created. How large is the `count_vecs` object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 57)\n"
     ]
    }
   ],
   "source": [
    "print(count_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be 3 rows (one for each of the documents) and 57 columns (one for each word in the vocabulary). These will generally be *sparse* because most documents do not contain most words. So hopefully whoever wrote this code was clever enough to store it as a sparse matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 2],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [2, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vecs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorious! They were! Good on them. Play around with the `count_vecs` object a bit and see if you can tell what type of sparse format that we've discussed in class they are using.\n",
    "\n",
    "For ease of working with the matrix, however, here we will simply use a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vecs = count_vecs.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize arrays for the term frequency, inverse document frequency and the TF-IDF scores. Note that the set-up from lecture has the first index of TF and TF-IDF referring to terms instead of documents, which is the transpose of the way the `CountVectorizer` encoded our documents.\n",
    "\n",
    "*(Yes, there are methods to directly compute the TF-IDF scores, but it's good to practice first and make sure we understand what they're doing.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.zeros(np.transpose(count_vecs).shape)\n",
    "idf = np.zeros(count_vecs.shape[1])\n",
    "tfidf = np.zeros(np.transpose(count_vecs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the term frequencies, for each term $i$ and each document $j$, we need to store in `tf[i,j]` the number of times term $i$ appears in document $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(count_vecs.shape[0]):\n",
    "    for i in range(count_vecs.shape[1]):\n",
    "        tf[i,j] = 0 # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly compute the inverse document frequency for each term by counting up the number of documents each term is in... which of course we should have done within the above loop structure but anyhoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is the number of documents\n",
    "N = 0 # TODO\n",
    "\n",
    "# IDF_i = ln(N/n_i), where n_i = number of documens term i is in.\n",
    "for i in range(count_vecs.shape[1]):\n",
    "    ni = 0 # TODO\n",
    "    idf[i] = 0 # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the TF-IDF scores as $w_{ij} = TF_{ij} \\times IDF_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(count_vecs.shape[1]):\n",
    "    for j in range(count_vecs.shape[0]):\n",
    "        tfidf[i,j] = 0 # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be a little silly to use for cosine distance similarity or an item measure, because there's basically no overlap except words we excluded with the `stop_words` input to our vocabulary.  But we could repeat the analysis for a user profile to determine words with similar linguistic profiles to the above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
